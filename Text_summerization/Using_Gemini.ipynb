{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdead43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0286ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe0139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a Large Language Model (LLM) | GeeksforGeeks\n",
      "Skip to content\n",
      "Courses\n",
      "DSA to Development\n",
      "Get IBM Certification\n",
      "Newly Launched!\n",
      "Master Django Framework\n",
      "Become AWS Certified\n",
      "For Working Professionals\n",
      "Interview 101: DSA & System Design\n",
      "Data Science Training Program\n",
      "JAVA Backend Development (Live)\n",
      "DevOps Engineering (LIVE)\n",
      "Data Structures & Algorithms in Python\n",
      "For Students\n",
      "Placement Preparation Course\n",
      "Data Science (Live)\n",
      "Data Structure & Algorithm-Self Paced (C++/JAVA)\n",
      "Master Competitive Programming (Live)\n",
      "Full Stack Development with React & Node JS (Live)\n",
      "Full Stack Development\n",
      "Data Science Program\n",
      "All Courses\n",
      "Tutorials\n",
      "Data Structures & Algorithms\n",
      "ML & Data Science\n",
      "Interview Corner\n",
      "Programming Languages\n",
      "Web Development\n",
      "CS Subjects\n",
      "DevOps And Linux\n",
      "School Learning\n",
      "Practice\n",
      "GfG 160: Daily DSA\n",
      "Problem of the Day\n",
      "Practice Coding Problems\n",
      "GfG SDE Sheet\n",
      "Python\n",
      "R Language\n",
      "Python for Data Science\n",
      "NumPy\n",
      "Pandas\n",
      "OpenCV\n",
      "Data Analysis\n",
      "ML Math\n",
      "Machine Learning\n",
      "NLP\n",
      "Deep Learning\n",
      "Deep Learning Interview Questions\n",
      "Machine Learning\n",
      "ML Projects\n",
      "ML Interview Questions\n",
      "Sign In\n",
      "▲\n",
      "Open In App\n",
      "Next Article:\n",
      "What are Language Models in NLP?\n",
      "What is a Large Language Model (LLM)\n",
      "Last Updated :\n",
      "22 Jan, 2025\n",
      "Comments\n",
      "Improve\n",
      "Suggest changes\n",
      "Like Article\n",
      "Like\n",
      "Report\n",
      "Large Language Models (LLMs) represent a breakthrough in artificial intelligence, employing neural network techniques with extensive parameters for advanced language processing.\n",
      "This article explores the evolution, architecture, applications, and challenges of LLMs, focusing on their impact in the field of Natural Language Processing (NLP).\n",
      "What are Large Language Models(LLMs)?\n",
      "A\n",
      "large language model\n",
      "is a type of artificial intelligence algorithm that applies neural network techniques with lots of parameters to process and understand human languages or text using self-supervised learning techniques. Tasks like text generation, machine translation, summary writing, image generation from texts, machine coding, chat-bots, or Conversational AI are applications of the Large Language Model.\n",
      "Examples of such LLM models are Chat GPT by open AI, BERT (Bidirectional Encoder Representations from Transformers) by Google, etc.\n",
      "There are many techniques that were tried to perform natural language-related tasks but the LLM is purely based on the\n",
      "deep learning\n",
      "methodologies. LLM (Large language model) models are highly efficient in capturing the complex entity relationships in the text at hand and can generate the text using the semantic and syntactic of that particular language in which we wish to do so.\n",
      "If we talk about the size of the advancements in the\n",
      "GPT (Generative Pre-trained Transformer)\n",
      "model only then:\n",
      "GPT-1\n",
      "which was released in 2018 contains 117 million parameters having 985 million words.\n",
      "GPT-2\n",
      "which was released in 2019 contains 1.5 billion parameters.\n",
      "GPT-3\n",
      "which was released in 2020 contains 175 billion parameters. Chat GPT is also based on this model as well.\n",
      "GPT-4\n",
      "model is released in the early 2023 and it is likely to contain trillions of parameters.\n",
      "GPT-4 Turbo\n",
      "was introduced in late 2023, optimized for speed and cost-efficiency, but its parameter count remains unspecified.\n",
      "How do Large Language Models work?\n",
      "Large Language Models (LLMs) operate on the principles of deep learning, leveraging neural network architectures to process and understand human languages.\n",
      "These models, are trained on vast datasets using\n",
      "self-supervised learning\n",
      "techniques. The core of their functionality lies in the intricate patterns and relationships they learn from diverse language data during training. LLMs consist of multiple layers, including feedforward layers, embedding layers, and attention layers. They employ attention mechanisms, like self-attention, to weigh the importance of different tokens in a sequence, allowing the model to capture dependencies and relationships.\n",
      "Architecture of LLM\n",
      "Large Language Model's (LLM) architecture is determined by a number of factors, like the objective of the specific model design, the available computational resources, and the kind of language processing tasks that are to be carried out by the LLM. The general architecture of LLM consists of many layers such as the feed forward layers, embedding layers, attention layers. A text which is embedded inside is collaborated together to generate predictions.\n",
      "Important components to influence Large Language Model architecture:\n",
      "Model Size and Parameter Count\n",
      "input representations\n",
      "Self-Attention Mechanisms\n",
      "Training Objectives\n",
      "Computational Efficiency\n",
      "Decoding and Output Generation\n",
      "Transformer-Based LLM Model Architectures\n",
      "Transformer\n",
      "-based models, which have revolutionized natural language processing tasks, typically follow a general architecture that includes the following components:\n",
      "Input Embeddings:\n",
      "The input text is tokenized into smaller units, such as words or sub-words, and each token is embedded into a continuous vector representation. This embedding step captures the semantic and syntactic information of the input.\n",
      "Positional Encoding:\n",
      "Positional encoding is added to the input embeddings to provide information about the positions of the tokens because transformers do not naturally encode the order of the tokens. This enables the model to process the tokens while taking their sequential order into account.\n",
      "Encoder:\n",
      "Based on a neural network technique, the encoder analyses the input text and creates a number of hidden states that protect the context and meaning of text data. Multiple encoder layers make up the core of the transformer architecture. Self-attention mechanism and feed-forward neural network are the two fundamental sub-components of each encoder layer.\n",
      "Self-Attention Mechanism:\n",
      "Self-attention enables the model to weigh the importance of different tokens in the input sequence by computing attention scores. It allows the model to consider the dependencies and relationships between different tokens in a context-aware manner.\n",
      "Feed-Forward Neural Network:\n",
      "After the self-attention step, a feed-forward neural network is applied to each token independently. This network includes fully connected layers with non-linear activation functions, allowing the model to capture complex interactions between tokens.\n",
      "Decoder Layers:\n",
      "In some transformer-based models, a decoder component is included in addition to the encoder. The decoder layers enable autoregressive generation, where the model can generate sequential outputs by attending to the previously generated tokens.\n",
      "Multi-Head Attention:\n",
      "Transformers often employ multi-head attention, where self-attention is performed simultaneously with different learned attention weights. This allows the model to capture different types of relationships and attend to various parts of the input sequence simultaneously.\n",
      "Layer Normalization:\n",
      "Layer normalization is applied after each sub-component or layer in the transformer architecture. It helps stabilize the learning process and improves the model's ability to generalize across different inputs.\n",
      "Output Layers:\n",
      "The output layers of the transformer model can vary depending on the specific task. For example, in language modeling, a linear projection followed by SoftMax activation is commonly used to generate the probability distribution over the next token.\n",
      "It's important to keep in mind that the actual architecture of transformer-based models can change and be enhanced based on particular research and model creations. To fulfill different tasks and objectives, several models like GPT, BERT, and T5 may integrate more components or modifications.\n",
      "Popular Large Language Models\n",
      "Now let's look at some of the famous LLMs which has been developed and are up for inference.\n",
      "GPT-3:\n",
      "GPT 3 is developed by OpenAI, stands for Generative Pre-trained Transformer 3. This model powers ChatGPT and is widely recognized for its ability to generate human-like text across a variety of applications.\n",
      "BERT:\n",
      "It is created by Google, is commonly used for natural language processing tasks and generating text embeddings, which can also be utilized for training other models.\n",
      "RoBERTa:\n",
      "RoBERTa is\n",
      "an advanced version of BERT, stands for Robustly Optimized BERT Pretraining Approach. Developed by Facebook AI Research, it enhances the performance of the transformer architecture.\n",
      "BLOOM:\n",
      "It is the first multilingual LLM, designed collaboratively by multiple organizations and researchers. It follows an architecture similar to GPT-3, enabling diverse language-based tasks.\n",
      "For implementation details, these models are available on open-source platforms like Hugging Face and OpenAI for Python-based applications.\n",
      "Large Language Models Use Cases\n",
      "Code Generation\n",
      ": LLMs can generate accurate code based on user instructions for specific tasks.\n",
      "Debugging and Documentation\n",
      ": They assist in identifying code errors, suggesting fixes, and even automating project documentation.\n",
      "Question Answering\n",
      ": Users can ask both casual and complex questions, receiving detailed, context-aware responses.\n",
      "Language Translation and Correction\n",
      ": LLMs can translate text between over 50 languages and correct grammatical errors.\n",
      "Prompt-Based Versatility\n",
      ": By crafting creative prompts, users can unlock endless possibilities, as LLMs excel in one-shot and zero-shot learning scenarios.\n",
      "Use cases of LLM are not limited to the above-mentioned one has to be just creative enough to write better prompts and you can make these models do a variety of tasks as they are trained to perform tasks on one-shot learning and zero-shot learning methodologies as well. Due to this only Prompt Engineering is a totally new and hot topic in academics for people who are looking forward to using ChatGPT-type models extensively.\n",
      "Applications of Large Language Models\n",
      "LLMs, such as GPT-3, have a wide range of applications across various domains. Few of them are:\n",
      "Natural Language Understanding (NLU):\n",
      "Large language models power advanced chatbots capable of engaging in natural conversations.\n",
      "They can be used to create intelligent virtual assistants for tasks like scheduling, reminders, and information retrieval.\n",
      "Content Generation:\n",
      "Creating human-like text for various purposes, including content creation, creative writing, and storytelling.\n",
      "Writing code snippets based on natural language descriptions or commands.\n",
      "Language Translation\n",
      ": Large language models can aid in translating text between different languages with improved accuracy and fluency.\n",
      "Text Summarization\n",
      ": Generating concise summaries of longer texts or articles.\n",
      "Sentiment Analysis\n",
      ": Analyzing and understanding sentiments expressed in social media posts, reviews, and comments.\n",
      "Difference Between NLP and LLM\n",
      "NLP is Natural Language Processing, a field of artificial intelligence (AI). It consists of the development of the algorithms. NLP is a broader field than LLM, which consists of algorithms and techniques. NLP rules two approaches i.e. Machine learning and the analyze language data. Applications of NLP are-\n",
      "Automotive routine task\n",
      "Improve search\n",
      "Search engine optimization\n",
      "Analyzing and organizing large documents\n",
      "Social Media Analytics.\n",
      "while on the other hand, LLM is a Large Language Model, and is more specific to human- like text, providing content generation, and personalized recommendations.\n",
      "What are the Advantages of Large Language Models?\n",
      "Large Language Models (LLMs) come with several advantages that contribute to their widespread adoption and success in various applications:\n",
      "LLMs can perform\n",
      "zero-shot learning\n",
      ", meaning they can generalize to tasks for which they were not explicitly trained. This capability allows for adaptability to new applications and scenarios without additional training.\n",
      "LLMs\n",
      "efficiently handle vast amounts of data\n",
      ", making them suitable for tasks that require a deep understanding of extensive text corpora, such as language translation and document summarization.\n",
      "LLMs can be\n",
      "fine-tuned\n",
      "on specific datasets or domains, allowing for continuous learning and adaptation to specific use cases or industries.\n",
      "LLMs\n",
      "enable the automation\n",
      "of various language-related tasks, from code generation to content creation, freeing up human resources for more strategic and complex aspects of a project.\n",
      "Challenges in Training of Large Language Models\n",
      "High Costs\n",
      ": Training LLMs requires significant financial investment, with millions of dollars needed for large-scale computational power.\n",
      "Time-Intensive\n",
      ": Training takes months, often involving human intervention for fine-tuning to achieve optimal performance.\n",
      "Data Challenges\n",
      ": Obtaining large text datasets is difficult, and concerns about the legality of data scraping for commercial purposes have arisen.\n",
      "Environmental Impact\n",
      ": Training a single LLM from scratch can produce carbon emissions equivalent to the lifetime emissions of five cars, raising serious environmental concerns.\n",
      "Conclusion\n",
      "Due to the challenges faced in training LLM transfer learning is promoted heavily to get rid of all of the challenges discussed above. LLM has the capability to bring revolution in the AI-powered application but the advancements in this field seem a bit difficult because just increasing the size of the model may increase its performance but after a particular time a saturation in the performance will come and the challenges to handle these models will be bigger than the performance boost achieved by further increasing the size of the models.\n",
      "Comment\n",
      "More info\n",
      "Advertise with us\n",
      "Next Article\n",
      "What are Language Models in NLP?\n",
      "A\n",
      "abhishekm482g\n",
      "Follow\n",
      "Improve\n",
      "Article Tags :\n",
      "Data Science\n",
      "Machine Learning\n",
      "NLP\n",
      "AI-ML-DS\n",
      "data-science\n",
      "ChatGPT\n",
      "+2 More\n",
      "Practice Tags :\n",
      "Machine Learning\n",
      "Similar Reads\n",
      "Top 20 LLM (Large Language Models)\n",
      "Large Language Model commonly known as an LLM, refers to a neural network equipped with billions of parameters and trained extensively on extensive datasets of unlabeled text. This training typically involves self-supervised or semi-supervised learning techniques. In this article, we explore about T\n",
      "15+ min read\n",
      "What is LLMOps (Large Language Model Operations)?\n",
      "LLMOps involves the strategies and techniques for overseeing the lifespan of large language models (LLMs) in operational environments. LLMOps ensure that LLMs are efficiently utilized for various natural language processing tasks, from fine-tuning to deployment and ongoing maintenance, in order to e\n",
      "8 min read\n",
      "Fine Tuning Large Language Model (LLM)\n",
      "Large Language Models (LLMs) have dramatically transformed natural language processing (NLP), excelling in tasks like text generation, translation, summarization, and question-answering. However, these models may not always be ideal for specific domains or tasks. To address this, fine-tuning is perf\n",
      "13 min read\n",
      "What are Language Models in NLP?\n",
      "Language models are a fundamental component of natural language processing (NLP) and computational linguistics. They are designed to understand, generate, and predict human language. These models analyze the structure and use of language to perform tasks such as machine translation, text generation,\n",
      "9 min read\n",
      "Gemma vs. Gemini vs. LLM (Large Language Model)\n",
      "Artificial Intelligence (AI) has witnessed exponential growth, with language models at the forefront of many transformative applications. Three key players in this space, Gemma, Gemini, and LLMs (Large Language Models), represent cutting-edge advancements in AI-driven conversational agents and data\n",
      "5 min read\n",
      "LLM vs GPT : Comparing Large Language Models and GPT\n",
      "In recent years, the field of natural language processing (NLP) has made tremendous strides, largely due to the development of large language models (LLMs) and, more specifically, the Generative Pre-trained Transformer (GPT) series. Both LLMs and GPTs have transformed how machines understand and gen\n",
      "4 min read\n",
      "Fine-Tuning Large Language Models (LLMs) Using QLoRA\n",
      "Fine-tuning large language models (LLMs) is used for adapting LLM's to specific tasks, improving their accuracy and making them more efficient. However full fine-tuning of LLMs can be computationally expensive and memory-intensive. QLoRA (Quantized Low-Rank Adapters) is a technique used to significa\n",
      "5 min read\n",
      "Future of Large Language Models\n",
      "In the last few years, the development of artificial intelligence has been in significant demand, with the emergence of Large Language Models (LLMs). This streamlined model entails advanced machine learning methods, has transformed natural language procedures, and is expected to revolutionize the fu\n",
      "8 min read\n",
      "Large Language Models (LLMs) vs Transformers\n",
      "In recent years, advancements in artificial intelligence have led to the development of sophisticated models that are capable of understanding and generating human-like text. Two of the most significant innovations in this space are Large Language Models (LLMs) and Transformers. While they are often\n",
      "7 min read\n",
      "What is PaLM 2: Google's Large Language Model Explained\n",
      "PaLM 2 is a strong large language model that Google has developed to break new ground in the capabilities of AI in understanding and creation. PaLM 2 is an upgraded version of the earlier version of PaLM and is more efficient in understanding and translating language and can even reason in some case\n",
      "7 min read\n",
      "Like\n",
      "Corporate & Communications Address:\n",
      "A-143, 7th Floor, Sovereign Corporate Tower, Sector- 136, Noida, Uttar Pradesh (201305)\n",
      "Registered Address:\n",
      "K 061, Tower K, Gulshan Vivante Apartment, Sector 137, Noida, Gautam Buddh Nagar, Uttar Pradesh, 201305\n",
      "Advertise with us\n",
      "Company\n",
      "About Us\n",
      "Legal\n",
      "Privacy Policy\n",
      "In Media\n",
      "Contact Us\n",
      "Advertise with us\n",
      "GFG Corporate Solution\n",
      "Placement Training Program\n",
      "Languages\n",
      "Python\n",
      "Java\n",
      "C++\n",
      "PHP\n",
      "GoLang\n",
      "SQL\n",
      "R Language\n",
      "Android Tutorial\n",
      "Tutorials Archive\n",
      "DSA\n",
      "Data Structures\n",
      "Algorithms\n",
      "DSA for Beginners\n",
      "Basic DSA Problems\n",
      "DSA Roadmap\n",
      "Top 100 DSA Interview Problems\n",
      "DSA Roadmap by Sandeep Jain\n",
      "All Cheat Sheets\n",
      "Data Science & ML\n",
      "Data Science With Python\n",
      "Data Science For Beginner\n",
      "Machine Learning\n",
      "ML Maths\n",
      "Data Visualisation\n",
      "Pandas\n",
      "NumPy\n",
      "NLP\n",
      "Deep Learning\n",
      "Web Technologies\n",
      "HTML\n",
      "CSS\n",
      "JavaScript\n",
      "TypeScript\n",
      "ReactJS\n",
      "NextJS\n",
      "Bootstrap\n",
      "Web Design\n",
      "Python Tutorial\n",
      "Python Programming Examples\n",
      "Python Projects\n",
      "Python Tkinter\n",
      "Python Web Scraping\n",
      "OpenCV Tutorial\n",
      "Python Interview Question\n",
      "Django\n",
      "Computer Science\n",
      "Operating Systems\n",
      "Computer Network\n",
      "Database Management System\n",
      "Software Engineering\n",
      "Digital Logic Design\n",
      "Engineering Maths\n",
      "Software Development\n",
      "Software Testing\n",
      "DevOps\n",
      "Git\n",
      "Linux\n",
      "AWS\n",
      "Docker\n",
      "Kubernetes\n",
      "Azure\n",
      "GCP\n",
      "DevOps Roadmap\n",
      "System Design\n",
      "High Level Design\n",
      "Low Level Design\n",
      "UML Diagrams\n",
      "Interview Guide\n",
      "Design Patterns\n",
      "OOAD\n",
      "System Design Bootcamp\n",
      "Interview Questions\n",
      "Inteview Preparation\n",
      "Competitive Programming\n",
      "Top DS or Algo for CP\n",
      "Company-Wise Recruitment Process\n",
      "Company-Wise Preparation\n",
      "Aptitude Preparation\n",
      "Puzzles\n",
      "School Subjects\n",
      "Mathematics\n",
      "Physics\n",
      "Chemistry\n",
      "Biology\n",
      "Social Science\n",
      "English Grammar\n",
      "Commerce\n",
      "World GK\n",
      "GeeksforGeeks Videos\n",
      "DSA\n",
      "Python\n",
      "Java\n",
      "C++\n",
      "Web Development\n",
      "Data Science\n",
      "CS Subjects\n",
      "@GeeksforGeeks, Sanchhaya Education Private Limited\n",
      ",\n",
      "All rights reserved\n",
      "We use cookies to ensure you have the best browsing experience on our website. By using our site, you\n",
      "        acknowledge that you have read and understood our\n",
      "Cookie Policy\n",
      "&\n",
      "Privacy Policy\n",
      "Got It !\n",
      "Improvement\n",
      "Suggest changes\n",
      "Suggest Changes\n",
      "Help us improve. Share your suggestions to enhance the article. Contribute your expertise and make a difference in the GeeksforGeeks portal.\n",
      "Create Improvement\n",
      "Enhance the article with your expertise. Contribute to the GeeksforGeeks community and help create better learning resources for all.\n",
      "Suggest Changes\n",
      "min 4 words, max Words Limit:1000\n",
      "Thank You!\n",
      "Your suggestions are valuable to us.\n",
      "What kind of Experience do you want to share?\n",
      "Interview Experiences\n",
      "Admission Experiences\n",
      "Career Journeys\n",
      "Work Experiences\n",
      "Campus Experiences\n",
      "Competitive Exam Experiences\n"
     ]
    }
   ],
   "source": [
    "data = Website(\"https://www.geeksforgeeks.org/large-language-model-llm/\")\n",
    "print(data.title)\n",
    "print(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f814007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article provides an overview of Large Language Models (LLMs), a significant advancement in AI. It explains that LLMs are AI algorithms utilizing neural networks with numerous parameters to process and understand human language through self-supervised learning. Examples include Chat GPT and BERT. The article discusses the evolution of LLMs, highlighting the increasing number of parameters in GPT models over the years.\n",
      "\n",
      "It details how LLMs work, focusing on deep learning principles, neural network architectures, and self-attention mechanisms. The article also covers the architecture of LLMs, including components like input embeddings, positional encoding, encoder/decoder layers, and multi-head attention.\n",
      "\n",
      "Furthermore, the article lists popular LLMs such as GPT-3, BERT, RoBERTa, and BLOOM, along with their use cases in code generation, debugging, question answering, and language translation. It also outlines applications in Natural Language Understanding, content generation, language translation, and text summarization.\n",
      "\n",
      "The article differentiates between NLP (a broad field) and LLMs (a specific type of model). It highlights the advantages of LLMs, such as zero-shot learning, handling vast amounts of data, fine-tuning capabilities, and automation of language-related tasks. Finally, it acknowledges the challenges in training LLMs, including high costs, time-intensive processes, data challenges, and environmental impact. The article concludes by emphasizing the importance of transfer learning and addressing concerns about the saturation of performance with increasing model size.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "response = model.generate_content(f\"Summarize the following article:\\n{data.text}\")\n",
    "\n",
    "print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
