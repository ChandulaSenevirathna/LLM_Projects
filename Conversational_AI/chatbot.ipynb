{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9555b7",
   "metadata": {},
   "source": [
    "### Conversational AI (Chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca49359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba75fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91bcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98e7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01bb00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gemini(prompt):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",  # or \"gemini-1.5-pro\"\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1477cb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=message_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf5f15",
   "metadata": {},
   "source": [
    "### Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf6c9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = \"You are a helpful assistant that responds in markdown\"  # Optional system instruction\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "\n",
    "    stream = model.generate_content(prompt, stream=True)\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            result += chunk.text\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3074b168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down quantum computing in a way that's (hopefully) easy to understand:\n",
       "\n",
       "**The Core Idea: It's a different way to calculate things.**\n",
       "\n",
       "Think of regular computers (like your phone or laptop) as light switches:\n",
       "\n",
       "*   **Regular Computers (Classical Computers):**  A light switch can be either ON (representing a 1) or OFF (representing a 0).  These 0s and 1s are called **bits**.  All calculations are done using these bits that are either 0 or 1.  It's like following a set of rules using only \"yes\" or \"no\" answers.\n",
       "\n",
       "*   **Quantum Computers:** Now imagine a dimmer switch.  It can be ON, OFF, or *anywhere in between*.  Think of it like a dial that can be at 0, 1, or a value somewhere in between. This \"in-between\" state is the key. Instead of bits, quantum computers use **qubits** (quantum bits). A qubit can be a 0, a 1, or **both 0 and 1 *at the same time***. This is called **superposition**.\n",
       "\n",
       "**Key Quantum Concepts Explained:**\n",
       "\n",
       "1.  **Superposition:** Imagine flipping a coin. Before it lands, it's *kind of* both heads and tails at the same time. A qubit is like that coin spinning in the air. It's in a combination of 0 and 1 until we \"look\" at it (measure it), at which point it \"collapses\" into either a 0 or a 1.  This ability to be in multiple states simultaneously allows quantum computers to explore many possibilities at once.\n",
       "\n",
       "2.  **Entanglement:** Imagine two of our special coins.  These coins are linked in a spooky way. If you flip one and it lands on heads, you *instantly* know the other one will land on tails (even if they are miles apart!).  This is entanglement.  Entangled qubits are linked together, and their fates are intertwined.  Measuring one entangled qubit instantly tells you something about the other, no matter the distance. This allows quantum computers to perform calculations in a coordinated way.\n",
       "\n",
       "**So, what does this *mean*?**\n",
       "\n",
       "Because qubits can be in multiple states at once (superposition) and can be linked together (entanglement), quantum computers can:\n",
       "\n",
       "*   **Explore many possibilities simultaneously:**  Instead of trying options one at a time (like a regular computer), a quantum computer can explore *all* the options at the same time.  This is like searching a maze by splitting yourself into many copies and exploring every path at once.\n",
       "*   **Solve certain problems much faster:** For specific types of problems, this \"parallel processing\" can lead to a massive speedup compared to regular computers.\n",
       "\n",
       "**What are Quantum Computers good at?**\n",
       "\n",
       "Quantum computers are *not* going to replace your laptop. They're good for very specific types of problems that are incredibly difficult for regular computers, such as:\n",
       "\n",
       "*   **Drug discovery and materials science:** Simulating molecules to design new drugs or materials.\n",
       "*   **Cryptography:** Breaking existing encryption algorithms (and creating new, quantum-resistant ones).\n",
       "*   **Optimization:** Finding the best solution from a vast number of possibilities (e.g., optimizing traffic flow, financial modeling, logistics).\n",
       "*   **Machine Learning:** Developing new and more powerful machine learning algorithms.\n",
       "\n",
       "**Why aren't we all using quantum computers already?**\n",
       "\n",
       "*   **They are very difficult to build:** Qubits are extremely fragile and easily disturbed by the environment (heat, vibrations, etc.). Maintaining their quantum state is a huge technological challenge.\n",
       "*   **They are expensive:** Building and maintaining quantum computers requires specialized equipment and expertise.\n",
       "*   **They are still in early stages of development:**  The field is rapidly advancing, but quantum computers are not yet ready for widespread use.  The number of qubits is relatively small, and they are prone to errors.\n",
       "*   **Programming is hard:**  You can't just run your regular programs on a quantum computer.  You need to write special quantum algorithms.\n",
       "\n",
       "**In a Nutshell:**\n",
       "\n",
       "Quantum computing uses the weird laws of quantum mechanics to perform calculations in a fundamentally different way than regular computers.  This allows them to potentially solve certain problems much faster, but they are still in early stages of development and are not a replacement for your everyday computer. They're more like specialized tools for specific, very complex problems.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for partial in stream_gemini(\"Explain quantum computing in simple terms.\"):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(partial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5370e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=gr.Markdown(),\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510c67",
   "metadata": {},
   "source": [
    "### Chat Bot With History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311de565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini_with_history(message, history):\n",
    "        \n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for user_msg, bot_msg in history:\n",
    "        prompt += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
    "    prompt += f\"User: {message}\\nAssistant:\"\n",
    "\n",
    "    stream = model.generate_content(prompt, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield history + [(message, response)], \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a6d9719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chandula\\AppData\\Local\\Temp\\ipykernel_15184\\1344445376.py:7: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Conversation\", show_copy_button=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_chat():\n",
    "    return [], \"\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as view:\n",
    "    gr.Markdown(\"## 💬 Chat Bot with History\")\n",
    "\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\", show_copy_button=True)\n",
    "    msg = gr.Textbox(label=\"Your message:\", lines=2, placeholder=\"Type your message here...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        send_btn = gr.Button(\"Send\")\n",
    "        clear_btn = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(stream_gemini_with_history, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "    send_btn.click(stream_gemini_with_history, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "\n",
    "    clear_btn.click(clear_chat, outputs=[chatbot, msg])\n",
    "\n",
    "view.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
