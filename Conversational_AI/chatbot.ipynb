{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9555b7",
   "metadata": {},
   "source": [
    "### Conversational AI (Chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca49359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba75fc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91bcd663",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98e7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bb00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gemini(prompt):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",  # or \"gemini-1.5-pro\"\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1477cb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=message_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf5f15",
   "metadata": {},
   "source": [
    "### Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = \"You are a helpful assistant that responds in markdown\"  # Optional system instruction\n",
    "\n",
    "def stream_gemini(prompt):\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "\n",
    "    stream = model.generate_content(prompt, stream=True)\n",
    "\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            result += chunk.text\n",
    "            yield result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3074b168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Quantum Computing Explained Simply\n",
       "\n",
       "Regular computers store information as bits, which are like switches that can be either on (1) or off (0).  Quantum computers use **qubits**.  Qubits are much more versatile. Because of the weirdness of quantum mechanics, they can be:\n",
       "\n",
       "* **0 or 1:** Just like a regular bit.\n",
       "* **Both 0 and 1 at the same time:** This is called **superposition**. Imagine a coin spinning in the air â€“ it's neither heads nor tails until it lands.\n",
       "* **Entangled:** Multiple qubits can be linked together in a way that their fates are intertwined.  Measuring one instantly tells you the state of the others, no matter how far apart they are!\n",
       "\n",
       "\n",
       "This allows quantum computers to explore many possibilities simultaneously.  Think of searching a maze: a regular computer tries each path one by one. A quantum computer can explore *all* paths at once, finding the exit much faster.\n",
       "\n",
       "**Here's the analogy:**\n",
       "\n",
       "Imagine you're searching for a specific grain of sand on a beach.\n",
       "\n",
       "* **Classical computer:** Checks each grain of sand one by one.\n",
       "* **Quantum computer:** Checks all grains of sand simultaneously.\n",
       "\n",
       "**However, it's important to note:**\n",
       "\n",
       "* Quantum computers aren't meant to replace regular computers. They're good at specific tasks, like drug discovery, materials science, and breaking certain types of encryption.\n",
       "* They're very difficult and expensive to build and maintain.  They require extremely low temperatures and are highly sensitive to noise.\n",
       "* They're still in their early stages of development.\n",
       "\n",
       "\n",
       "In short, quantum computing leverages the bizarre rules of quantum mechanics to perform calculations in a fundamentally different and potentially much faster way than classical computers for specific problems.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for partial in stream_gemini(\"Explain quantum computing in simple terms.\"):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(partial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5370e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=stream_gemini,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
    "    outputs=gr.Markdown(),\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70510c67",
   "metadata": {},
   "source": [
    "### Chat Bot With History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311de565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gemini_with_history(message, history):\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.0-flash\",\n",
    "        system_instruction=system_message\n",
    "    )\n",
    "    \n",
    "    prompt = \"\"\n",
    "    for user_msg, bot_msg in history:\n",
    "        prompt += f\"User: {user_msg}\\nAssistant: {bot_msg}\\n\"\n",
    "    prompt += f\"User: {message}\\nAssistant:\"\n",
    "\n",
    "    stream = model.generate_content(prompt, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield history + [(message, response)], \"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
